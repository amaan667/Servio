# Log Aggregation Service

This document describes the log aggregation strategy and implementation for the Servio platform.

## Table of Contents

1. [Overview](#overview)
2. [Architecture](#architecture)
3. [Log Sources](#log-sources)
4. [Log Format](#log-format)
5. [Implementation](#implementation)
6. [Configuration](#configuration)
7. [Querying and Analysis](#querying-and-analysis)
8. [Alerting](#alerting)
9. [Best Practices](#best-practices)

## Overview

The Servio platform uses a centralized log aggregation service to collect, store, and analyze logs from all components of the system. This provides:

- **Centralized Logging**: All logs in one place for easy analysis
- **Real-time Monitoring**: Live log streaming and alerting
- **Search and Filter**: Powerful query capabilities
- **Retention Policies**: Configurable log retention
- **Security**: Secure log storage and access control

## Architecture

```
┌─────────────────────────────────────────────────────────────────┐
│                         Log Sources                              │
│  ┌──────────┐  ┌──────────┐  ┌──────────┐  ┌──────────┐       │
│  │  App     │  │  API     │  │  Worker  │  │  DB      │       │
│  │  Logs    │  │  Logs    │  │  Logs    │  │  Logs    │       │
│  └────┬─────┘  └────┬─────┘  └────┬─────┘  └────┬─────┘       │
└───────┼────────────┼────────────┼────────────┼────────────────┘
        │            │            │            │
        └────────────┴────────────┴────────────┘
                     │
                     ▼
        ┌─────────────────────────┐
        │   Log Collector        │
        │   (Vector/Fluentd)      │
        └───────────┬─────────────┘
                    │
                    ▼
        ┌─────────────────────────┐
        │   Log Storage           │
        │   (Elasticsearch/ClickHouse)│
        └───────────┬─────────────┘
                    │
                    ▼
        ┌─────────────────────────┐
        │   Log Analysis          │
        │   (Grafana/Kibana)      │
        └─────────────────────────┘
```

## Log Sources

### Application Logs

Generated by the Next.js application:

```typescript
// lib/structured-logger.ts
import { createLogger } from './structured-logger';

const logger = createLogger('app');

// Info log
logger.info('User logged in', {
  userId: '123',
  email: 'user@example.com',
  ip: '192.168.1.1'
});

// Error log
logger.error('Payment failed', {
  orderId: '456',
  error: 'Insufficient funds',
  amount: 100
});

// Performance log
logger.performance('API request', {
  endpoint: '/api/orders',
  duration: 150,
  status: 200
});
```

### API Logs

Generated by API routes:

```typescript
// app/api/orders/route.ts
import { createLogger } from '@/lib/structured-logger';

const logger = createLogger('api');

export async function POST(request: Request) {
  const startTime = Date.now();

  try {
    const body = await request.json();
    logger.info('Creating order', { body });

    const order = await createOrder(body);

    logger.info('Order created successfully', {
      orderId: order.id,
      duration: Date.now() - startTime
    });

    return Response.json(order);
  } catch (error) {
    logger.error('Failed to create order', {
      error: error.message,
      duration: Date.now() - startTime
    });

    return Response.json(
      { error: 'Failed to create order' },
      { status: 500 }
    );
  }
}
```

### Database Logs

Generated by Supabase/PostgreSQL:

```sql
-- Enable query logging
ALTER DATABASE servio SET log_statement = 'all';
ALTER DATABASE servio SET log_duration = on;
ALTER DATABASE servio SET log_min_duration_statement = 1000; -- Log queries > 1s
```

### Worker Logs

Generated by background workers:

```typescript
// lib/queue.ts
import { createLogger } from '@/lib/structured-logger';

const logger = createLogger('worker');

queue.process('order-fulfillment', async (job) => {
  logger.info('Processing order fulfillment', {
    jobId: job.id,
    orderId: job.data.orderId
  });

  try {
    await fulfillOrder(job.data.orderId);

    logger.info('Order fulfillment completed', {
      jobId: job.id,
      orderId: job.data.orderId
    });
  } catch (error) {
    logger.error('Order fulfillment failed', {
      jobId: job.id,
      orderId: job.data.orderId,
      error: error.message
    });

    throw error;
  }
});
```

## Log Format

All logs follow a structured JSON format:

```json
{
  "timestamp": "2024-01-15T10:30:45.123Z",
  "level": "info",
  "service": "app",
  "environment": "production",
  "venueId": "venue_123",
  "userId": "user_456",
  "message": "User logged in",
  "context": {
    "email": "user@example.com",
    "ip": "192.168.1.1",
    "userAgent": "Mozilla/5.0..."
  },
  "metadata": {
    "requestId": "req_789",
    "traceId": "trace_abc",
    "spanId": "span_def"
  }
}
```

### Log Levels

- **debug**: Detailed information for debugging
- **info**: General informational messages
- **warn**: Warning messages for potential issues
- **error**: Error messages for failures
- **fatal**: Critical errors requiring immediate attention

### Standard Fields

| Field | Type | Description |
|-------|------|-------------|
| timestamp | string | ISO 8601 timestamp |
| level | string | Log level (debug, info, warn, error, fatal) |
| service | string | Service name (app, api, worker, db) |
| environment | string | Environment (development, staging, production) |
| venueId | string | Venue ID (if applicable) |
| userId | string | User ID (if applicable) |
| message | string | Log message |
| context | object | Additional context data |
| metadata | object | Request/trace metadata |

## Implementation

### Option 1: Vector + Elasticsearch

**Vector** is a high-performance log collector and router.

#### Installation

```bash
# Install Vector
curl --proto '=https' --tlsv1.2 -sSf https://sh.vector.dev | sh

# Or using Docker
docker pull timberio/vector:latest
```

#### Configuration

```toml
# vector.toml
[sources.app_logs]
type = "file"
include = ["/var/log/servio/app.log"]
read_from = "beginning"

[sources.api_logs]
type = "file"
include = ["/var/log/servio/api.log"]
read_from = "beginning"

[sources.worker_logs]
type = "file"
include = ["/var/log/servio/worker.log"]
read_from = "beginning"

[transforms.parse_logs]
type = "remap"
inputs = ["app_logs", "api_logs", "worker_logs"]
source = '''
  . = parse_json!(.message)
  .timestamp = parse_timestamp!(.timestamp, "%Y-%m-%dT%H:%M:%S%.fZ") ?? now()
  .environment = "${ENVIRONMENT:-development}"
'''

[sinks.elasticsearch]
type = "elasticsearch"
inputs = ["parse_logs"]
endpoint = "${ELASTICSEARCH_ENDPOINT:-http://localhost:9200}"
index = "servio-logs-%Y-%m-%d"
bulk.action = "create"
bulk.size_mb = 10
bulk.timeout_secs = 30

[sinks.stdout]
type = "console"
inputs = ["parse_logs"]
encoding.codec = "json"
```

#### Docker Compose

```yaml
# docker-compose.yml
version: '3.8'

services:
  vector:
    image: timberio/vector:latest
    volumes:
      - ./vector.toml:/etc/vector/vector.toml
      - ./logs:/var/log/servio
    environment:
      - ENVIRONMENT=${ENVIRONMENT:-development}
      - ELASTICSEARCH_ENDPOINT=http://elasticsearch:9200
    depends_on:
      - elasticsearch

  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms512m -Xmx512m"
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data

  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    ports:
      - "5601:5601"
    environment:
      - ELASTICSEARCH_HOSTS=http://elasticsearch:9200
    depends_on:
      - elasticsearch

volumes:
  elasticsearch_data:
```

### Option 2: Fluentd + ClickHouse

**Fluentd** is a popular log collector with many plugins.

#### Installation

```bash
# Install Fluentd
gem install fluentd

# Or using Docker
docker pull fluent/fluentd:v1.16-1
```

#### Configuration

```xml
<!-- fluent.conf -->
<source>
  @type tail
  path /var/log/servio/*.log
  pos_file /var/log/fluentd-servio.pos
  tag servio.*
  <parse>
    @type json
  </parse>
</source>

<filter servio.**>
  @type record_transformer
  <record>
    hostname "#{Socket.gethostname}"
    environment "#{ENV['ENVIRONMENT'] || 'development'}"
  </record>
</filter>

<match servio.**>
  @type clickhouse
  host clickhouse
  port 8123
  database servio_logs
  table logs
  <buffer>
    @type file
    path /var/log/fluentd-buffers/servio
    flush_mode interval
    flush_interval 5s
  </buffer>
</match>
```

#### ClickHouse Schema

```sql
CREATE DATABASE IF NOT EXISTS servio_logs;

CREATE TABLE IF NOT EXISTS servio_logs.logs (
  timestamp DateTime,
  level String,
  service String,
  environment String,
  venueId String,
  userId String,
  message String,
  context String,
  metadata String,
  hostname String,
  date Date MATERIALIZED toDate(timestamp)
) ENGINE = MergeTree()
PARTITION BY toYYYYMM(timestamp)
ORDER BY (timestamp, service, level)
TTL timestamp + INTERVAL 30 DAY;
```

### Option 3: Cloud-Native Solutions

#### AWS CloudWatch

```typescript
// lib/logger/cloudwatch.ts
import { CloudWatchLogs } from '@aws-sdk/client-cloudwatch-logs';

const cloudwatch = new CloudWatchLogs({
  region: process.env.AWS_REGION,
});

export async function sendToCloudWatch(logs: any[]) {
  const params = {
    logGroupName: '/servio/production',
    logStreamName: `app-${Date.now()}`,
    logEvents: logs.map(log => ({
      message: JSON.stringify(log),
      timestamp: Date.now(),
    })),
  };

  await cloudwatch.putLogEvents(params);
}
```

#### Google Cloud Logging

```typescript
// lib/logger/gcp.ts
import { Logging } from '@google-cloud/logging';

const logging = new Logging({
  projectId: process.env.GOOGLE_CLOUD_PROJECT,
});

export async function sendToGCP(logs: any[]) {
  const log = logging.log('servio');
  const entries = logs.map(log => ({
    resource: { type: 'global' },
    severity: log.level.toUpperCase(),
    jsonPayload: log,
  }));

  await log.write(entries);
}
```

#### Azure Monitor

```typescript
// lib/logger/azure.ts
import { DefaultAzureCredential } from '@azure/identity';
import { LogsIngestionClient } from '@azure/monitor-ingestion';

const credential = new DefaultAzureCredential();
const client = new LogsIngestionClient(
  process.env.AZURE_DATA_COLLECTION_ENDPOINT,
  credential
);

export async function sendToAzure(logs: any[]) {
  await client.upload(
    process.env.AZURE_DATA_COLLECTION_RULE_ID,
    process.env.AZURE_STREAM_NAME,
    logs
  );
}
```

## Configuration

### Environment Variables

```bash
# Log Aggregation
LOG_LEVEL=info
LOG_FORMAT=json
LOG_OUTPUT=stdout,file

# Elasticsearch
ELASTICSEARCH_ENDPOINT=http://localhost:9200
ELASTICSEARCH_INDEX=servio-logs
ELASTICSEARCH_USERNAME=elastic
ELASTICSEARCH_PASSWORD=changeme

# ClickHouse
CLICKHOUSE_HOST=localhost
CLICKHOUSE_PORT=8123
CLICKHOUSE_DATABASE=servio_logs
CLICKHOUSE_USER=default
CLICKHOUSE_PASSWORD=

# CloudWatch
AWS_REGION=us-east-1
AWS_CLOUDWATCH_LOG_GROUP=/servio/production

# Google Cloud
GOOGLE_CLOUD_PROJECT=my-project
GOOGLE_CLOUD_LOG_NAME=servio

# Azure
AZURE_DATA_COLLECTION_ENDPOINT=https://my-endpoint.ingest.monitor.azure.com
AZURE_DATA_COLLECTION_RULE_ID=my-rule-id
AZURE_STREAM_NAME=servio-logs
```

### Log Retention

Configure retention policies based on log importance:

```sql
-- Elasticsearch Index Lifecycle Management
PUT _ilm/policy/servio-logs-policy
{
  "policy": {
    "phases": {
      "hot": {
        "actions": {
          "rollover": {
            "max_size": "50GB",
            "max_age": "1d"
          }
        }
      },
      "warm": {
        "min_age": "7d",
        "actions": {
          "shrink": {
            "number_of_shards": 1
          }
        }
      },
      "delete": {
        "min_age": "30d",
        "actions": {
          "delete": {}
        }
      }
    }
  }
}
```

## Querying and Analysis

### Kibana Queries

```json
// Find all errors in the last hour
{
  "query": {
    "bool": {
      "must": [
        { "match": { "level": "error" } },
        { "range": { "timestamp": { "gte": "now-1h" } } }
      ]
    }
  }
}

// Find slow API requests
{
  "query": {
    "bool": {
      "must": [
        { "match": { "service": "api" } },
        { "range": { "context.duration": { "gte": 1000 } } }
      ]
    }
  }
}

// Find failed payments
{
  "query": {
    "bool": {
      "must": [
        { "match": { "message": "Payment failed" } },
        { "range": { "timestamp": { "gte": "now-24h" } } }
      ]
    }
  }
}
```

### ClickHouse Queries

```sql
-- Find all errors in the last hour
SELECT *
FROM servio_logs.logs
WHERE level = 'error'
  AND timestamp >= now() - INTERVAL 1 HOUR
ORDER BY timestamp DESC;

-- Find slow API requests
SELECT
  timestamp,
  service,
  message,
  context.duration
FROM servio_logs.logs
WHERE service = 'api'
  AND JSONExtractFloat(context, 'duration') > 1000
ORDER BY timestamp DESC;

-- Count errors by service
SELECT
  service,
  count() as error_count
FROM servio_logs.logs
WHERE level = 'error'
  AND timestamp >= now() - INTERVAL 24 HOUR
GROUP BY service
ORDER BY error_count DESC;

-- Find failed payments
SELECT *
FROM servio_logs.logs
WHERE message LIKE '%Payment failed%'
  AND timestamp >= now() - INTERVAL 24 HOUR
ORDER BY timestamp DESC;
```

## Alerting

### Log-Based Alerts

```yaml
# alertmanager.yml
groups:
  - name: log_alerts
    rules:
      - alert: HighErrorRate
        expr: rate(logs_total{level="error"}[5m]) > 10
        for: 5m
        labels:
          severity: critical
        annotations:
          summary: "High error rate detected"
          description: "Error rate is {{ $value }} errors/min"

      - alert: DatabaseSlowQueries
        expr: rate(logs_total{service="db",message=~"%slow query%"}[5m]) > 5
        for: 5m
        labels:
          severity: warning
        annotations:
          summary: "Slow database queries detected"
          description: "Slow query rate is {{ $value }} queries/min"

      - alert: PaymentFailures
        expr: rate(logs_total{message=~"%Payment failed%"}[5m]) > 1
        for: 2m
        labels:
          severity: critical
        annotations:
          summary: "Payment failures detected"
          description: "Payment failure rate is {{ $value }} failures/min"
```

## Best Practices

### 1. Structured Logging

Always use structured logging with consistent fields:

```typescript
// Good
logger.info('Order created', {
  orderId: '123',
  userId: '456',
  amount: 100,
  currency: 'USD'
});

// Bad
logger.info('Order 123 created by user 456 for $100');
```

### 2. Log Levels

Use appropriate log levels:

```typescript
// Debug: Detailed information for debugging
logger.debug('Processing order item', { itemId: '123' });

// Info: General informational messages
logger.info('Order created', { orderId: '123' });

// Warn: Warning messages for potential issues
logger.warn('Order amount exceeds limit', { orderId: '123', amount: 10000 });

// Error: Error messages for failures
logger.error('Payment failed', { orderId: '123', error: 'Insufficient funds' });

// Fatal: Critical errors requiring immediate attention
logger.fatal('Database connection lost', { error: 'Connection timeout' });
```

### 3. Contextual Information

Include relevant context in logs:

```typescript
logger.info('API request', {
  method: request.method,
  url: request.url,
  userId: session.userId,
  venueId: session.venueId,
  requestId: request.headers.get('x-request-id'),
  userAgent: request.headers.get('user-agent')
});
```

### 4. Performance Logging

Log performance metrics:

```typescript
const startTime = Date.now();

// ... do work ...

logger.performance('Operation completed', {
  operation: 'createOrder',
  duration: Date.now() - startTime,
  success: true
});
```

### 5. Error Logging

Log errors with full context:

```typescript
try {
  await createOrder(orderData);
} catch (error) {
  logger.error('Failed to create order', {
    error: error.message,
    stack: error.stack,
    orderId: orderData.id,
    userId: orderData.userId,
    venueId: orderData.venueId
  });
  throw error;
}
```

### 6. Sensitive Data

Never log sensitive information:

```typescript
// Bad
logger.info('User logged in', {
  email: 'user@example.com',
  password: 'secret123'
});

// Good
logger.info('User logged in', {
  userId: '123',
  email: 'u***@example.com'
});
```

### 7. Log Rotation

Configure log rotation to prevent disk space issues:

```bash
# /etc/logrotate.d/servio
/var/log/servio/*.log {
  daily
  rotate 30
  compress
  delaycompress
  missingok
  notifempty
  create 0644 servio servio
}
```

## References

- [Vector Documentation](https://vector.dev/docs/)
- [Fluentd Documentation](https://docs.fluentd.org/)
- [Elasticsearch Documentation](https://www.elastic.co/guide/en/elasticsearch/reference/current/)
- [ClickHouse Documentation](https://clickhouse.com/docs/en/)
- [AWS CloudWatch Logs](https://docs.aws.amazon.com/AmazonCloudWatch/latest/logs/)
- [Google Cloud Logging](https://cloud.google.com/logging/docs)
- [Azure Monitor](https://docs.microsoft.com/en-us/azure/azure-monitor/)
